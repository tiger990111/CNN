{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Building a Neural Network\n",
    "\n",
    "## Date:  Sunday, October 13th 2019\n",
    "\n",
    "#### Authors: David Sondak and Pavlos Protopapas\n",
    "\n",
    "In the first lecture, you were introduced to the basics of neural networks.  In today's lab, you will create a simple neural network build some intuition about how it works.  The basic ideas will be demonstrated with code snippets and exercises.  You will explore the affect of different activation functions, the importance of the neuron weights and biases, and the affect of multiple interacting neurons.\n",
    "\n",
    "First, we import some basic `python` libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "1. Make sure you clone the current lab.  I may have made last-minute changes.\n",
    "2. You should be able to use `Jupyter` notebooks.  The recommended way of doing this is to install [`Anaconda`](https://www.anaconda.com/download/#macos).  If you don't have `Anaconda` now and aren't sure how to run a `Jupyter` notebook from your local machine, I have provided an alternative option for you.\n",
    "3. The alternative to running `Jupyter` notebook on your local machine is to use the separate lab repo: [Labs Repo](https://github.com/dsondak/dswcc_fall_2018).  Click on the `Binder` badge.  After a few minutes, you will have a `Jupyter` notebook environment with the current lab running in your browser.\n",
    "\n",
    "In addition to the `Jupyter` notebooks, you are expected to have some familiarity with `Python`.  I am happy to answer some questions during lab, but I may also refer you to extra reading if it takes us too far away from our current topic.\n",
    "\n",
    "### A Workflow Suggestion\n",
    "From time to time, I will push some last-minute changes to a lab.  I recommend the following workflow:\n",
    "1. Clone the repo before each session.\n",
    "2. Make a copy of the current lab called `Lab1-working.ipynb` and do all your work in the working notebook.\n",
    "3. Whenever I make a last-minute change, you won't run the risk of losing all your lab work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting with a Single Node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with a single node (also called a neuron).\n",
    "\n",
    "![perceptron](figs/perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some inputs $x$, which get combined into an auxilliary variable $z$.  The auxilliary variable is passed through the activation function $\\sigma\\left(z\\right)$ and the result is the output.\n",
    "\n",
    "Here is another image showing each step.\n",
    "\n",
    "![](figs/expanded-perceptron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the inputs are linearly combined according to some weights $w$ and a bias $b$.  This transformation is also sometimes called and *affine* transformation.  The perceptron transforms the weighted inputs according to the rule of the activation function.  For a single perceptron, the output $y$ is just the output from the perceptron.  The linear transformation and activation of the neuron occur within a single *layer* of the network (shown in the dotted box)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "The task is to approximate (or learn) a function $f\\left(x\\right)$ given some input $x$.  For demonstration purposes, the function we will try to learn is a Gaussian function $$f\\left(x\\right) = \\exp\\left(-x^{2}\\right).$$  Even though we represent the input $x$ as a vector on the computer, you should think of it as a single input.  For example, we're not passing $x$ and $y$ into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5.0, 5.0, 500) # input points\n",
    "f = np.exp(-x*x) # The real function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the single-layer, single neuron network gives us.  We have a couple of choices to make:\n",
    "1. We must choose some weights and some biases\n",
    "2. We must choose an activation function\n",
    "\n",
    "For now, we will manually specify the weights and biases.\n",
    "\n",
    "We choose a *sigmoid* activation function $$\\sigma\\left(x\\right) = \\dfrac{1}{1 + e^{-z}}.$$  What are the limits $\\displaystyle\\lim_{z\\to\\infty}\\sigma\\left(z\\right)$ and $\\displaystyle\\lim_{z\\to-\\infty}\\sigma\\left(z\\right)$?  Actually, the sigmoid we have here is called the *logistic* function.  Sigmoids are really a family of functions and the logistic function is just one member in that family.\n",
    "\n",
    "There are other activation functions as well.  For example:\n",
    "* Rectified linear unit:  $$\\sigma\\left(z\\right) = \\text{max}\\left(0, z\\right)$$\n",
    "* Hyperbolic tangent:  $$\\sigma\\left(z\\right) = \\tanh\\left(z\\right)$$\n",
    "\n",
    "Choosing the correct activation function is a really big deal.  Notice that the activation functions we've mentioned so far have the feature that they \"turn on\" at some point and \"saturate\" at some point.\n",
    "\n",
    "We will examine different activation functions next week.\n",
    "\n",
    "Okay, enough talk.  Let's code up the sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually specify the weight and bias\n",
    "w = -4.5 # weight\n",
    "b = 3.0 # bias\n",
    "\n",
    "# Perceptron output\n",
    "z = w * x + b # Linear transformation\n",
    "h = 1.0 / (1.0 + np.exp(-z)) # Sigmoid activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we plot the activation function and the true function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(11,7)) # create axes object\n",
    "\n",
    "# Plot\n",
    "ax.plot(x, f, lw=4, label=r'True function')\n",
    "ax.plot(x, h, lw=4, label=r'Single Neuron')\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=24) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=24)\n",
    "\n",
    "ax.tick_params(labelsize=24) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=24, loc=1) # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction looks awful.  But we expected that.  The single perceptron simply turns the output on and off at some point, but that's about it.  We see that the neuron is on until about $x=0$ at which point it abruptly turns off.  It's able to get \"close\" to the true function just by luck.  Otherwise, it has nothing in common with the true function.\n",
    "\n",
    "What do you think will happen if you change $w$ and $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "#### Part 1\n",
    "Write two `Python` functions:\n",
    "1. The first function should return an affine transformation of the data for a single perceptron.  Here's the required interface:\n",
    "```python\n",
    "def affine(x, w, b):\n",
    "    \"\"\"Return affine transformation of x\n",
    "    \n",
    "    INPUTS\n",
    "    ======\n",
    "    x: A numpy array of points in x\n",
    "    w: A float representing the weight of the perceptron\n",
    "    b: A float representing the bias of the perceptron\n",
    "    \n",
    "    RETURN\n",
    "    ======\n",
    "    z: A numpy array of points after the affine transformation\n",
    "       z = wx + b\n",
    "    \"\"\"\n",
    "    \n",
    "    # Code goes here\n",
    "    return z\n",
    "```\n",
    "2. The second function should return the sigmoid activation function.  Here's the required interface:\n",
    "```python\n",
    "def sigmoid(z):\n",
    "    # Code goes here\n",
    "    return h\n",
    "```\n",
    "\n",
    "#### Part 2\n",
    "* Using your two functions, recreate the example from class\n",
    "* Try to change the weight and bias to get a better fit\n",
    "\n",
    "\n",
    "#### Comments\n",
    "* We say that the activation occurs when $\\sigma = \\dfrac{1}{2}$.  We can show that this corresponds to $x = -\\dfrac{b}{w}$.\n",
    "* The \"steepness\" of the sigmoid is controlled by $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "#### Important Observation\n",
    "Notice that we wrote the output as `sigmoid(affine(x))`.  This was not a coincidence.  It looks like a composition of functions.  In fact, that is what a neural network is doing.  It's building up an approximation to a function by creating a composition of functions.  For example, a composition of three functions would be written as $$\\varphi_{3}\\left(\\varphi_{2}\\left(\\varphi_{1}\\left(x\\right)\\right)\\right).$$\n",
    "\n",
    "What happens if we play with the weights and biases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = [-5.0, 0.1, 5.0] # Create a list of weights\n",
    "b = [0.0, -1.0, 1.0] # Create a list of biases\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(15,10))\n",
    "ax.plot(x, f, lw=4, ls='-.', label='True function')\n",
    "for wi, bi in zip(w, b):\n",
    "    h = sigmoid(affine(x, wi, bi))\n",
    "    ax.plot(x, h, lw=4, label=r'$w = {{{0}}}$, $b = {{{1}}}$'.format(wi,bi))\n",
    "\n",
    "ax.set_title('Single neuron network', fontsize=24)\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=24) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=24)\n",
    "\n",
    "ax.tick_params(labelsize=24) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=24, loc='best') # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't do an exhaustive search of the weights and biases, but it sure looks like this single perceptron is never going to match the actual function.  Again, we shouldn't be suprised about this.  The output layer of the network is simple the logistic function, which can only have so much flexibility.\n",
    "\n",
    "Not all hope is lost!  We may be able to make our network more flexible by using more nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Perceptrons in a Single Layer\n",
    "It appears that a single neuron is somewhat limited in what it can accomplish.  What if we expand the number of neurons in our network?  We have two obvious choices here.  One option is to add depth to the network by putting neurons next to each other.  We won't take that path yet.  The other option is to stack neurons on top of each other.  Now the network has some width, but is still only one layer deep.\n",
    "\n",
    "The following figure shows a single-layer network with two nodes in one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](figs/multiple-perceptrons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some observations\n",
    "1. We still have a single input in this case.  Note that this is not necessary in general.  We're just keeping things simple with a single input for now.\n",
    "2. Each node (or neuron) has a weight and bias associated with it.  An affine transformation is performed for each node.\n",
    "3. Both nodes use the same activation function form $\\sigma\\left(\\cdot\\right)$ on their respective inputs.\n",
    "4. The outputs of the nodes must be combined to give the overall output of the network.  There are a variety of ways of accomplishing this.  In the current example, we just take a linear combination of the node outputs to produce the actual prediction.  Notice that now we have weights and biases at the output too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens in this case.  First, we just compute the outputs of each neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5.0, 5.0, 500) # input points\n",
    "f = np.exp(-x*x) # data\n",
    "\n",
    "w = np.array([1.5, -2.5])\n",
    "b = np.array([0.4, -2.0])\n",
    "\n",
    "# Affine transformations\n",
    "z1 = w[0] * x + b[0]\n",
    "z2 = w[1] * x + b[1]\n",
    "\n",
    "# Node outputs\n",
    "h1 = 1.0 / (1.0 + np.exp(-z1))\n",
    "h2 = 1.0 / (1.0 + np.exp(-z2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot things and see what they look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(14,10))\n",
    "\n",
    "ax.plot(x, f, lw=4, ls = '-.', label='True function')\n",
    "ax.plot(x, h1, lw=4, label='First neuron')\n",
    "ax.plot(x, h2, lw=4, label='Second neuron')\n",
    "\n",
    "# Set title\n",
    "ax.set_title('Comparison of Neuron Outputs', fontsize=24)\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=24) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=24)\n",
    "\n",
    "ax.tick_params(labelsize=24) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=24, loc='best') # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we expected.  Some sigmoids.  Of course, to get the network prediction we must combine these two sigmoid curves somehow.  First we'll just add $h_{1}$ and $h_{2}$ without any weights to see what happens.\n",
    "\n",
    "#### Note\n",
    "We are **not** doing classification here.  We are trying to predict an actual function.  The sigmoid activation is convenient when doing classification because you need to go from $0$ to $1$.  However, when learning a function, we don't have as good of a reason to choose a sigmoid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network output\n",
    "wout = np.ones(2) # Set the output weights to unity to begin\n",
    "bout = 0.0 # No bias yet\n",
    "yout = wout[0] * h1 + wout[1] * h2 + bout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(14,10))\n",
    "\n",
    "ax.plot(x, f, ls='-.', lw=4, label=r'True function')\n",
    "ax.plot(x, yout, lw=4, label=r'$y_{out} = h_{1} + h_{2}$')\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=24) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=24)\n",
    "\n",
    "ax.tick_params(labelsize=24) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=24, loc='best') # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "* The network prediction is still really bad.\n",
    "* *But*, it is pretty sophisticated.  We just have two neurons, but we get some pretty interesting behavior.\n",
    "* We should be cautiously optimistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't do anything with the output weights.  Those are probably important.  Now let's see what happens when we change the weights on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network output\n",
    "wout = np.array([-1.5, -1.5])\n",
    "bout = np.array(1.5)\n",
    "\n",
    "yout = wout[0] * h1 + wout[1] * h2 + bout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(14,10))\n",
    "\n",
    "ax.plot(x, f, lw=4, ls = '-.', label='True function')\n",
    "ax.plot(x, yout, lw=4, label=r'$y_{out} = w_{1}^{o}h_{1} + w_{2}^{o}h_{2} + b^{o}$')\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=20) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=20)\n",
    "\n",
    "ax.tick_params(labelsize=24) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=24, loc=1) # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very cool!  The two nodes interact with each other to produce a pretty complicated-looking function.  It still doesn't match the true function, but now we have some hope.  In fact, it's starting to look a little bit like a Gaussian!\n",
    "\n",
    "I bet we can do better.  There are three obvious options at this point:\n",
    "1. Change the number of nodes\n",
    "2. Change the activation functions\n",
    "3. Change the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Mathematical Notation\n",
    "Before proceeding, let's learn a more succint way of doing the calculations.  If you have a network with a lot of nodes, then you probably don't want to manually determine the output of each node.  It will take forever.  Instead, you can package the computations up using a more compact notation.  We'll illustrate the ideas with the two-node network.\n",
    "\n",
    "Suppose we have a single input $x$ to a single-layer two-node network.  We can store the weights from each node in a vector $\\mathbf{w} \\in \\mathbb{R}^{2}$.  Similarly, we can store the biases from each node in a vector $\\mathbf{b} \\in \\mathbb{R}^{2}$.  The affine transformation is then written as $$\\mathbf{z} = \\mathbf{w}x + \\mathbf{b}$$ where the usual laws of vector addition and multiplication by a scalar apply.  Of course, we have that $\\mathbf{z} \\in \\mathbb{R}^{2}$ as well.  Next we evaluate the output from each node.  Formally, we write $$\\mathbf{h} = \\sigma\\left(\\mathbf{z}\\right)$$ where, once again, $\\mathbf{h}\\in\\mathbb{R}^{2}$.  Moreover, it is *understood* that $\\sigma$ operates on each individual element of $\\mathbf{z}$ separately.  If we denote each component of $\\mathbf{z}$ by $z_{j}, \\ j = 1, 2$ then we can write $$h_{j} = \\sigma\\left(z_{j}\\right), \\quad j = 1, 2.$$\n",
    "\n",
    "Lastly, we must do something about the output layer.  Mathematically we write $$y_{out} = \\mathbf{w}_{out} \\cdot \\mathbf{h} + b_{out}$$ where $\\mathbf{w}_{out} \\in \\mathbb{R}^{2}$ and $b_{out} \\in \\mathbb{R}$.\n",
    "\n",
    "### Comments on the Implementation\n",
    "Mathematically, this all makes perfect sense.  There is a slight wrinkle when we get to the implementation.  The reason is that $x$ must be stored as a vector (or array) on the computer!  When we do that, we must be very careful in telling the computer how to perform the multiplications.\n",
    "\n",
    "#### Example A\n",
    "Suppose we have just stored three points in $x$ on the computer and store $x$ in an array.  Then we have \n",
    "\\begin{align}\n",
    "  x = \\begin{bmatrix} x_{1} \\ x_{2} \\ x_{3} \\end{bmatrix}.\n",
    "\\end{align}\n",
    "When we write $\\mathbf{w}x + \\mathbf{b}$ the computer actually doesn't know what multiplication we want to do.  If we treat $x$ as though it has dimension $3 \\times 1$ then simply multiplying $\\mathbf{w}$ by $x$ will result in a dimension mismatch error.\n",
    "\n",
    "#### Example B\n",
    "What if we're at the output of the network?  Then $\\mathbf{h}$ is stored as a $2 \\times \\text{num_points}$ array.  But $\\mathbf{w}_{out}$ is a $2\\times 1$ array!  To get the dot product right, we need to tell the computer to take the dot product on each column.  Here's the situation:\n",
    "\\begin{align}\n",
    "  \\mathbf{w}_{out} \\cdot \\mathbf{h} &= \n",
    "  \\begin{bmatrix}\n",
    "    w_{1}^{out} \\ \\ w_{2}^{out}\n",
    "  \\end{bmatrix}^{T}\n",
    "  \\begin{bmatrix}\n",
    "    h_{11} & h_{12} & h_{13} \\\\\n",
    "    h_{21} & h_{22} & h_{23}\n",
    "  \\end{bmatrix}\n",
    "  \\\\\n",
    "  &=\n",
    "  \\begin{bmatrix}\n",
    "    w_{1}^{out}h_{11} + w_{2}^{out}h_{21} \\qquad w_{1}^{out}h_{12} + w_{2}^{out}h_{22} \\qquad w_{1}^{out}h_{13} + w_{2}^{out}h_{23}\n",
    "  \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "To overcome these problems, we can use the `numpy` [`tensordot`](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.tensordot.html) function.  That function allows us to directly specify how to perform the multiplication.  I would encourage you all to play around with `tensordot` to get used to the syntax.\n",
    "\n",
    "The next example shows how to re-write our single-layer network using this succinct notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "Write a function that can handle the above situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes(x, w, b):\n",
    "    z = np.tensordot(w, x, axes=0) + b.reshape((len(w),-1))\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def output(h, wout, bout):\n",
    "    return np.tensordot(wout, h, axes=1) + bout\n",
    "\n",
    "h = nodes(x, w, b)\n",
    "yout = output(h, wout, bout)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(14,10))\n",
    "\n",
    "ax.plot(x, f, lw=4, label='True function')\n",
    "ax.plot(x, yout, lw=4, label=r'$y_{out} = w_{1}^{o}h_{1} + w_{2}^{o}h_{2} + b^{o}$')\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=20) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=20)\n",
    "\n",
    "ax.tick_params(labelsize=24) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=24, loc=1) # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the same result.  Good.  We're ready to try some more interesting things now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Number of Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a nice new function that allows us to use an arbitrary number of nodes in a single-layer network.  Let's see what happens when we use a bunch of nodes.  What do you think will happen?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In this exercise, you will use the functions just provided to change the number of nodes in a layer.  Here's your task.\n",
    "\n",
    "You are provided you with a code outline below.  Fill in the blanks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_nodes = [2, 4, 8, 16, 32]\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(14,10))\n",
    "\n",
    "ax.plot(x, f, lw=4, ls ='-.', label='True function')\n",
    "\n",
    "# Loop over list of nodes\n",
    "for N in num_nodes:\n",
    "    # Select node weights\n",
    "    w = np.random.uniform(-3.0, 3.0, N) # randomly generate weights from uniform distribution\n",
    "    b = # fill in\n",
    "    wo = # fill in\n",
    "    bo =  # fill in\n",
    "    \n",
    "    h = # fill in\n",
    "    yout = # fill in\n",
    "\n",
    "    ax.plot(x, yout, lw=4, label=r'$N = {{{0}}}$'.format(N)) # Plot each solution\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=20) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=20)\n",
    "\n",
    "ax.tick_params(labelsize=24) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(fontsize=24, loc=1) # Create a legend and make it big enough to read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a little disappointing.  We're getting more complex behavior in our predictions, but they're still barely matching the true function.  It could be our activation function choice (sigmoid).  Or (more likely) it's the weights that we are using.  Right now, we're just grabbing random weights.  Let's do a grid-search over the weights and biases and assess the affects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Weights\n",
    "To keep things simple, let's go back to the single layer, two-node network.\n",
    "\n",
    "We'll do a grid search.  This means that we'll create a grid of weights for the first node, a grid of weights for the second node, a grid of biases for the first node, and a grid of biases for the second node.  Then, we'll loop over every single combination of weights and biases in our grid.  This is a very greedy and impractical way of doing things.  The scaling is terrible.  With just two nodes, we're searching a four dimensional space.  There are much more sophisticated algorithms out there, but we won't discuss those today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w_one = np.array([-0.5, 1.5])\n",
    "w_two = np.array([-0.5, 1.5])\n",
    "\n",
    "b_one = np.array([-2.0, 2.0])\n",
    "b_two = np.array([-2.0, 2.0])\n",
    "\n",
    "# Manually specify some output weights and bias\n",
    "wout = np.array([-1.5, -1.5])\n",
    "bout = np.array(1.5)\n",
    "\n",
    "# Initiaize the plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(14,10))\n",
    "ax.plot(x, f, lw=8, label='True function')\n",
    "\n",
    "# Loop over the grid\n",
    "for w1 in w_one:\n",
    "    for w2 in w_two:\n",
    "        w = np.array([w1, w2]) # Get the weights for each node\n",
    "        for b1 in b_one:\n",
    "            for b2 in b_two:\n",
    "                b = np.array([b1, b2]) # Get the biases for each node\n",
    "                h = nodes(x, w, b) # Outputs of the nodes\n",
    "                yout = output(h, wout, bout) # Network output\n",
    "                \n",
    "                ax.plot(x, yout, lw=3, alpha=0.5) # Make a plot\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=20) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=20)\n",
    "ax.set_title('Results from Grid Search', fontsize=24)\n",
    "\n",
    "ax.tick_params(labelsize=24) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(loc=1, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a pretty expensive search and didn't learn very much.  In fact, we didn't even make a grid for the output weights and bias!\n",
    "\n",
    "Another way to do a search is to draw samples from a distribution.  We could draw $w_{1}$ from a normal distribution $10^{5}$ times and do the same for the other weights and biases.  Then we could compute the function from the single-layer two-node network and compare it with the actual function.  This is what we do in the next example.  \n",
    "\n",
    "We compare the two functions using the root mean square error, $$RMSE = \\sqrt{\\frac{1}{n_{points}}\\sum_{i=1}^{n_{points}}{\\left(f_{i} - y^{out}_{i}\\right)^{2}}}.$$  The weights and biases are drawn from a normal distribution centered about the points we found earlier.  The best result is stored and compared with the actual function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the weights and biases\n",
    "w = np.array([1.5, -2.5])\n",
    "b = np.array([0.4, -2.0])\n",
    "\n",
    "wout = np.array([-1.5, -1.5])\n",
    "bout = np.array(1.25)\n",
    "\n",
    "# Initiaize the plot\n",
    "fig, ax = plt.subplots(1,1, figsize=(14,10))\n",
    "\n",
    "rmse_min = 10^15 # Set a large RMSE to start\n",
    "for n in range(100000):\n",
    "    # Draw the weights and biases from a normal distribution\n",
    "    w1 = np.random.normal(w[0], np.sqrt(0.1 * w[0] * w[0]))\n",
    "    w2 = np.random.normal(w[1], np.sqrt(0.1 * w[1] * w[1]))\n",
    "    b1 = np.random.normal(b[0], np.sqrt(0.1 * b[0] * b[0]))\n",
    "    b2 = np.random.normal(b[1], np.sqrt(0.1 * b[1] * b[1]))\n",
    "\n",
    "    w1o = np.random.normal(wout[0], np.sqrt(0.1 * wout[0] * wout[0]))\n",
    "    w2o = np.random.normal(wout[1], np.sqrt(0.1 * wout[1] * wout[1]))\n",
    "    bo = np.random.normal(bout, np.sqrt(0.1 * bout * bout))\n",
    "    \n",
    "    # Store the weights and biases in the necessary forms\n",
    "    wnodes = np.array([w1, w2])\n",
    "    bnodes = np.array([b1, b2])\n",
    "    w_out = np.array([w1o, w2o])\n",
    "    \n",
    "    # Compute output of the network for these parameter values\n",
    "    h = nodes(x, wnodes, bnodes)\n",
    "    yout = output(h, w_out, bo)\n",
    "    \n",
    "    # Calculate the RMSE\n",
    "    err = yout - f\n",
    "    err2 = err * err\n",
    "    errbar = err2.mean()\n",
    "    rmse = np.sqrt(errbar)\n",
    "    \n",
    "    # If the RMSE decreased from the previous step, then save the \n",
    "    # parameters\n",
    "    if rmse < rmse_min:\n",
    "        w_star = wnodes\n",
    "        b_star = bnodes\n",
    "        wout_star = np.array([w1o, w2o])\n",
    "        bout_star = bo\n",
    "        \n",
    "        rmse_min = rmse\n",
    "\n",
    "print(\"The minimum RMSE was {0}\".format(rmse_min))\n",
    "\n",
    "# Recompute the best solution\n",
    "h = nodes(x, w_star, b_star)\n",
    "yout = output(h, wout_star, bout_star)\n",
    "\n",
    "# Now plot everything\n",
    "ax.plot(x, f, ls='-.', lw=4, label='True function')\n",
    "ax.plot(x, yout, lw=4, label='Best fit')\n",
    "\n",
    "# Create labels (very important!)\n",
    "ax.set_xlabel('$x$', fontsize=20) # Notice we make the labels big enough to read\n",
    "ax.set_ylabel('$y$', fontsize=20)\n",
    "ax.set_title('Results from random search', fontsize=24)\n",
    "\n",
    "ax.tick_params(labelsize=24) # Make the tick labels big enough to read\n",
    "\n",
    "ax.legend(loc=1, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and What's Ahead\n",
    "\n",
    "### Summary\n",
    "In this lab, we built up some intuition for what a neural network is doing.  The following concepts were introduced and illustrated:\n",
    "* Input\n",
    "* Nodes (also called neurons) and layers\n",
    "* Weights and biases\n",
    "* Activation functions\n",
    "* Output / prediction\n",
    "We explored how the outputs of individual nodes are combined to give a final prediction.  The sensitivity of the output on the node weights and biases was illustrated with examples.\n",
    "\n",
    "You should, however, have several lingering questions.  We didn't deeply explore the affect of different activation functions.  Also, there has got to be a better way of tuning the weights.  Randomly trying different weights didn't work so well and a grid search is way too expensive.  Plus, what's the deal with \"deep\" networks?  We played around with the width of a network, but what happens if we start putting nodes next to each other?\n",
    "\n",
    "### What's Ahead\n",
    "Next time, we will begin to explore some of these ideas.  We will dispense with writing our own neural network code for the time-being.  Instead, we will work with [`keras`](https://keras.io/): \"a high-level neural networks API\".  You will learn how to use `keras` to build bigger networks and how to actually train them using real data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
